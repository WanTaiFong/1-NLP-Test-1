{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import the pandas package, then use the \"read_csv\" function to read\n# the labeled training data\nimport pandas as pd       \ntrain = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\nprint (\"Train data imported\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(train.columns.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train[\"review\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import BeautifulSoup into your workspace\nfrom bs4 import BeautifulSoup             \n\n# Initialize the BeautifulSoup object on a single movie review     \nexample1 = BeautifulSoup(train[\"review\"][0])  \n\n# Print the raw review and then the output of get_text(), for \n# comparison\nprint (example1.get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# Use regular expressions to do a find-and-replace\nletters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n                      \" \",                   # The pattern to replace it with\n                      example1.get_text() )  # The text to search\nprint (letters_only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case = letters_only.lower()        # Convert to lower case\nwords = lower_case.split()               # Split into words\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n#nltk.download()  # Download text data sets, including stop words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can use nltk to get a list of stop words:\nfrom nltk.corpus import stopwords # Import the stop word list\nprint (stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will allow you to view the list of English-language stop words. To remove stop words from our movie review, do:\n# Remove stop words from \"words\"\nwords = [w for w in words if not w in stopwords.words(\"english\")]\nprint (words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to do stemming to group similar terms together\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create an object of class PorterStemmer\nporter = PorterStemmer()\nlancaster=LancasterStemmer()\n#proide a word to be stemmed\nprint(\"Porter Stemmer\")\nprint(porter.stem(\"cats\"))\nprint(porter.stem(\"trouble\"))\nprint(porter.stem(\"troubling\"))\nprint(porter.stem(\"troubled\"))\nprint(\"Lancaster Stemmer\")\nprint(lancaster.stem(\"cats\"))\nprint(lancaster.stem(\"trouble\"))\nprint(lancaster.stem(\"troubling\"))\nprint(lancaster.stem(\"troubled\"))\n\n# Cannot stem with a sentence\nprint(lancaster.stem(\"troubled trouble troubling\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because stemming has to be done word by word, not the whole sentence\nword_stem = []\nfor word in words:\n    word_stem.append(porter.stem(word))\nprint(word_stem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 5.5 Try, stemming with Porter\n    word_stem = []\n    for word in meaningful_words:\n        word_stem.append(porter.stem(word))\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    #return( \" \".join( meaningful_words ))\n    return( \" \".join( word_stem ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_review = review_to_words( train[\"review\"][0] )\nprint (clean_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the number of reviews based on the dataframe column size\nnum_reviews = train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\nprint (\"Cleaning and parsing the training set movie reviews...\\n\")\nclean_train_reviews = []\nfor i in range( 0, num_reviews ):\n    # If the index is evenly divisible by 1000, print a message\n    if( (i+1)%5000 == 0 ):\n        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n    clean_train_reviews.append(review_to_words(train[\"review\"][i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_train_reviews[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.  \nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of \n# strings.\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n# Numpy arrays are easy to work with, so convert the result to an \n# array\ntrain_data_features = train_data_features.toarray()\nprint (\"Creating the bag of words...\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_data_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the words in the vocabulary\nvocab = vectorizer.get_feature_names()\nprint (vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Sum up the counts of each vocabulary word\ndist = np.sum(train_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(vocab, dist):\n    print (count, tag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the test data\ntest = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\n\n# Verify that there are 25,000 rows and 2 columns\nprint (test.shape)\nprint(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an empty list and append the clean reviews one by one\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\nprint (\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    if( (i+1) % 5000 == 0 ):\n        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\nprint(test_data_features.shape)\nprint(test_data_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n#!pip install numpy==1.16.1\nimport numpy as np\n\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense, GRU, Embedding\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\nprint(\"finish import\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# User defined parameters\nbatch_size_user = 128\nembedding_size = 32\nnum_words = 5000\nmax_tokens = 5000\nepoch_user = 10\nprint(\"User defined parameters set\")\nprint(batch_size_user)\nprint(embedding_size)\nprint(num_words)\nprint(max_tokens)\nprint(epoch_user)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel = 0\nmodel = Sequential()\nmodel.add(Embedding(num_words, embedding_size, input_length=max_tokens))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data_features\ny_train = train[\"sentiment\"]\nprint(\"xy values set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nmodel.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random forest to make sentiment label predictions\n#result = forest.predict(test_data_features)\n\ny_test = model.predict(test_data_features)\nprint(y_test.shape)\nprint(sum(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\n\noutput = test\noutput = output.drop(columns=['review'])\noutput[\"sentiment\"] = y_test\nprint(output)\n#output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput.to_csv( \"model1.csv\", index=False, quoting=3 )\nprint(\"model1.csv printed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel2 = 0\nmodel2 = Sequential()\nmodel2.add(Embedding(num_words, embedding_size, input_length=max_tokens))\nmodel2.add(Flatten())\nmodel2.add(Dense(50, activation='relu'))\nmodel2.add(Dense(50, activation='relu'))\nmodel2.add(Dense(50, activation='relu'))\nmodel2.add(Dense(50, activation='relu'))\nmodel2.add(Dense(50, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = train_data_features\ny_train = train[\"sentiment\"]\nprint(\"xy values set\")\n\n# Fit the model\nmodel2.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n\n# Use the random forest to make sentiment label predictions\n#result = forest.predict(test_data_features)\n\ny_test = model2.predict(test_data_features)\nprint(y_test.shape)\nprint(sum(y_test))\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\n\noutput2 = test\noutput2 = output2.drop(columns=['review'])\noutput2[\"sentiment\"] = y_test\nprint(output2)\n#output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n\n# Use pandas to write the comma-separated output file\n#output.to_csv( \"model1.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput2.to_csv( \"model2.csv\", index=False, quoting=3 )\nprint(\"model2.csv printed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel3 = 0\nmodel3 = Sequential()\nmodel3.add(Embedding(num_words, embedding_size, input_length=max_tokens))\nmodel3.add(Flatten())\nmodel3.add(Dropout(0.2, input_shape=(160000,)))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(1, activation='sigmoid'))\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = train_data_features\ny_train = train[\"sentiment\"]\nprint(\"xy values set\")\n\n# Fit the model\nmodel3.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n\n# Use the random forest to make sentiment label predictions\n#result = forest.predict(test_data_features)\n\ny_test = model3.predict(test_data_features)\nprint(y_test.shape)\nprint(sum(y_test))\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\n\noutput3 = test\noutput3 = output3.drop(columns=['review'])\noutput3[\"sentiment\"] = y_test\nprint(output3)\n#output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n\n# Use pandas to write the comma-separated output file\n#output.to_csv( \"model1.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput3.to_csv( \"model3.csv\", index=False, quoting=3 )\nprint(\"model3.csv printed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel4 = 0\nmodel4 = Sequential()\nmodel4.add(Embedding(num_words, embedding_size, input_length=max_tokens))\nmodel4.add(Flatten())\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dropout(0.2))\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dropout(0.2))\nmodel4.add(Dense(50, activation='relu'))\nmodel4.add(Dense(1, activation='sigmoid'))\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = train_data_features\ny_train = train[\"sentiment\"]\nprint(\"xy values set\")\n\n# Fit the model\nmodel4.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n\n# Use the random forest to make sentiment label predictions\n#result = forest.predict(test_data_features)\n\ny_test = model4.predict(test_data_features)\nprint(y_test.shape)\nprint(sum(y_test))\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\n\noutput4 = test\noutput4 = output4.drop(columns=['review'])\noutput4[\"sentiment\"] = y_test\nprint(output4)\n#output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n\n# Use pandas to write the comma-separated output file\n#output.to_csv( \"model1.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput4.to_csv( \"model4.csv\", index=False, quoting=3 )\nprint(\"model4.csv printed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM layer expects inputs to have shape of (batch_size, timesteps, input_dim).\n# In keras you need to pass (timesteps, input_dim) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel5 = 0\nmodel5 = Sequential()\nmodel5.add(Embedding(num_words, embedding_size, input_length=max_tokens))\n#model5.add(Flatten())\n#model5.add(Dropout(0.2))\n#model5.add(GRU(units=64, input_shape=(125000000,1,1),return_sequences=True))\n#model5.add(GRU(units=32, return_sequences=True))\n#model5.add(GRU(units=16, return_sequences=True))\n#model5.add(GRU(units=8, return_sequences=True))\nmodel5.add(GRU(units=4))\nmodel5.add(Dense(1, activation='sigmoid'))\nmodel5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model5.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = train_data_features\n#y_train = train[\"sentiment\"]\n#print(\"xy values set\")\n#model5.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n\n#y_test = model5.predict(test_data_features)\n#print(y_test.shape)\n#print(sum(y_test))\n\n#output5 = test\n#output5 = output5.drop(columns=['review'])\n#output5[\"sentiment\"] = y_test\n#print(output5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel6 = 0\nmodel6 = Sequential()\nmodel6.add(Embedding(num_words, embedding_size, input_length=max_tokens))\nmodel6.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel6.add(MaxPooling1D(pool_size=2))\nmodel6.add(Flatten())\nmodel6.add(Dropout(0.2))\nmodel6.add(Dense(50, activation='relu'))\nmodel6.add(Dense(50, activation='relu'))\nmodel6.add(Dense(50, activation='relu'))\nmodel6.add(Dense(50, activation='relu'))\nmodel6.add(Dense(50, activation='relu'))\nmodel6.add(Dense(1, activation='sigmoid'))\nmodel6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model6.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To ensure results can be repeated, the same seed value should be used for all testing\nseed_value= 2019\n\nfrom numpy.random import seed\nseed(seed_value)\n# Additional seed value required to be set for tensorflow backend\nfrom tensorflow import set_random_seed\nset_random_seed(seed_value)\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nprint(\"Seed values re-set to 2019.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = train_data_features\ny_train = train[\"sentiment\"]\nprint(\"xy values set\")\n\n# Fit the model\nmodel6.fit(X_train, y_train, epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n\n# Use the random forest to make sentiment label predictions\n#result = forest.predict(test_data_features)\n\ny_test = model6.predict(test_data_features)\nprint(y_test.shape)\nprint(sum(y_test))\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\n\noutput6 = test\noutput6 = output6.drop(columns=['review'])\noutput6[\"sentiment\"] = y_test\nprint(output6)\n#output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":y_test} )\n\n# Use pandas to write the comma-separated output file\n#output.to_csv( \"model1.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pandas to write the comma-separated output file\noutput6.to_csv( \"model6.csv\", index=False, quoting=3 )\nprint(\"model6.csv printed\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}