{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1) NLP Test Tutorial 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmkNnsbTFsIw",
        "colab_type": "text"
      },
      "source": [
        "Following the TensorFlow Tutorial #20 from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6AkqM-tFsnA",
        "colab_type": "code",
        "outputId": "79250ea6-ee18-45f8-e002-08390e559ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GRU, Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5MHDmCGRM2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
        "#from tensorflow.python.keras.optimizers import Adam\n",
        "#from tensorflow.python.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBhLYJuAF-Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqsPNeBUGAKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpCztSOeGK23",
        "colab_type": "text"
      },
      "source": [
        "Install once per run, for import imdb to work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJmTJWtAGJrY",
        "colab_type": "code",
        "outputId": "b59cdf19-bba6-45bf-8af7-502854e4d80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!pip install imdbpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imdbpy in /usr/local/lib/python3.6/dist-packages (6.8)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.6/dist-packages (from imdbpy) (1.3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from imdbpy) (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBx3CFERGDLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxguREJZHAzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdbO = imdb.IMDb()\n",
        "\n",
        "object_methods = [method_name for method_name in dir(imdbO)\n",
        "                  if callable(getattr(imdbO, method_name))]\n",
        "print(object_methods)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1I3roKrIJEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "# load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPELOit1I47P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"X_train[0]\", X_train[0])\n",
        "print(\"y_train[0]\", y_train[0])\n",
        "print(\"X_test[0]\", X_test[0])\n",
        "print(\"y_test[0]\", y_test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eUv-au_Ivt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "print(\"X[0]\", X[0])\n",
        "print(\"y[0]\", y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEORteEtIlAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize size\n",
        "print(\"Training data: \")\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForGviOhJJW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize number of classes\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqpyzM9lJP_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize number of words\n",
        "print(\"Number of words: \")\n",
        "print(len(np.unique(np.hstack(X))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYD27ZbJuth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "plt.boxplot(result)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78fxKYz2iR2j",
        "colab_type": "text"
      },
      "source": [
        "# **Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1pvQBhFo2j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# User defined parameters\n",
        "batch_size_user = 128\n",
        "embedding_size = 32\n",
        "num_words = 5000\n",
        "max_tokens = 500\n",
        "epoch_user = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2YCM7ReLk3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4BN3KbFoua8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXLboqVGL3xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen=max_tokens)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne9ejqUDcIDy",
        "colab_type": "text"
      },
      "source": [
        "# **Preparing the models for anaylsis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyTM1kInMJYp",
        "colab_type": "code",
        "outputId": "82088f81-c493-47ee-d23f-bec8bb01d524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_size, input_length=max_tokens))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 16000)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 250)               4000250   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 4,160,501\n",
            "Trainable params: 4,160,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS-r3GuLOx3q",
        "colab_type": "text"
      },
      "source": [
        "Batch size affects the number of samples exposed to the network per run before updating it, more samples would take more memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3f-jSZtPI5i",
        "colab_type": "text"
      },
      "source": [
        "Increasing batch size trains the network faster as lesser updating is done which leads to lesser accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqRCspx4PEpi",
        "colab_type": "text"
      },
      "source": [
        "Conversely, decreasing batch size trains the network slower, but higher accuracy is achieved as there is more updating sessions with the same number of epochs. However risk overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwATkxybMXNo",
        "colab_type": "code",
        "outputId": "c9d733cb-4474-4efb-a441-735ac58533ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_user, batch_size=batch_size_user, verbose=1)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 32s 1ms/step - loss: 0.5084 - acc: 0.7087 - val_loss: 0.3318 - val_acc: 0.8552\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 30s 1ms/step - loss: 0.1887 - acc: 0.9292 - val_loss: 0.3033 - val_acc: 0.8720\n",
            "Accuracy: 87.20%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc3sbz69OpRZ",
        "colab_type": "text"
      },
      "source": [
        "2nd Model attempt, with RNN/GRU included before the NN portion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr9q3FPPPVUJ",
        "colab_type": "code",
        "outputId": "ab004e3f-abb0-40ee-9700-06b3ea668ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "model2.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))\n",
        "model2.add(GRU(units=16, return_sequences=True))\n",
        "model2.add(GRU(units=8, return_sequences=True))\n",
        "model2.add(GRU(units=4))\n",
        "model2.add(Dense(250, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "optimizer = Adam(lr=1e-3)\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model2.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_embedding (Embedding)  (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 500, 16)           2352      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 500, 8)            600       \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, 4)                 156       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 250)               1250      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 164,609\n",
            "Trainable params: 164,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDEGLA-VQRMy",
        "colab_type": "code",
        "outputId": "88816cef-f750-49d1-db19-9800704c84ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_user, batch_size=batch_size_user, verbose = 1)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 149s 6ms/step - loss: 0.5432 - acc: 0.7063 - val_loss: 0.3635 - val_acc: 0.8483\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 152s 6ms/step - loss: 0.2988 - acc: 0.8792 - val_loss: 0.3262 - val_acc: 0.8668\n",
            "Accuracy: 86.68%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5n3cKibSvHT",
        "colab_type": "text"
      },
      "source": [
        "When evaluating the 2nd model with the added RNN/GRU before the Dense network, the accuracy climb noticably has backward drops regularly as compared to the 1st model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeAKMFgzXZCJ",
        "colab_type": "text"
      },
      "source": [
        "Additional, a longer duration is required to train the 2nd model with RNN/GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q35Lrk8tZOhL",
        "colab_type": "code",
        "outputId": "e95b40c9-a8f5-4aa3-f4b6-fd8d05fb8b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# create the model\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(num_words, embedding_size, input_length=max_tokens))\n",
        "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model3.add(MaxPooling1D(pool_size=2))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(250, activation='relu'))\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8000)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 250)               2000250   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,163,605\n",
            "Trainable params: 2,163,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F0tS2iIbB6S",
        "colab_type": "code",
        "outputId": "ce7c3a7b-cd9c-4e40-9cd8-0ef683441ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_user, batch_size=batch_size_user, verbose = 1)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 33s 1ms/step - loss: 0.5014 - acc: 0.7188 - val_loss: 0.2820 - val_acc: 0.8830\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 32s 1ms/step - loss: 0.2197 - acc: 0.9142 - val_loss: 0.2689 - val_acc: 0.8879\n",
            "Accuracy: 88.79%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}